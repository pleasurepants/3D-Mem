=== JOB START ===
Sat Jun 21 09:00:58 PM CEST 2025
worker-minor-3
Sat Jun 21 21:00:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:65:00.0 Off |                  N/A |
| 23%   32C    P8               8W / 250W |      0MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:B3:00.0 Off |                  N/A |
| 23%   26C    P8               7W / 250W |      0MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
SLURM_JOB_ID: 75113
[INFO] CUDA_VISIBLE_DEVICES=0,1
[INFO] Starting vLLM (minicpm) server on GPU 0...
[INFO] Waiting for vLLM (minicpm) server to be ready...
  ... waiting (2s)
  ... waiting (4s)
  ... waiting (6s)
  ... waiting (8s)
  ... waiting (10s)
  ... waiting (12s)
  ... waiting (14s)
  ... waiting (16s)
  ... waiting (18s)
INFO 06-21 21:01:19 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (20s)
  ... waiting (22s)
  ... waiting (24s)
  ... waiting (26s)
  ... waiting (28s)
INFO 06-21 21:01:29 [api_server.py:1287] vLLM API server version 0.9.1
  ... waiting (30s)
INFO 06-21 21:01:30 [cli_args.py:309] non-default args: {'model': 'openbmb/MiniCPM-V-2_6', 'trust_remote_code': True, 'served_model_name': ['minicpm'], 'limit_mm_per_prompt': {'image': 20}}
  ... waiting (32s)
  ... waiting (34s)
  ... waiting (36s)
  ... waiting (38s)
  ... waiting (40s)
  ... waiting (42s)
  ... waiting (44s)
INFO 06-21 21:01:45 [config.py:823] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
WARNING 06-21 21:01:45 [config.py:3220] Your device 'NVIDIA GeForce GTX 1080 Ti' (with compute capability 6.1) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 06-21 21:01:45 [config.py:3271] Casting torch.bfloat16 to torch.float16.
WARNING 06-21 21:01:45 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
INFO 06-21 21:01:45 [api_server.py:265] Started engine process with PID 3415281
  ... waiting (46s)
WARNING 06-21 21:01:47 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
  ... waiting (48s)
  ... waiting (50s)
INFO 06-21 21:01:52 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (52s)
  ... waiting (54s)
INFO 06-21 21:01:56 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='openbmb/MiniCPM-V-2_6', speculative_config=None, tokenizer='openbmb/MiniCPM-V-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=minicpm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
  ... waiting (56s)
INFO 06-21 21:01:57 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 06-21 21:01:57 [cuda.py:324] Using XFormers backend.
  ... waiting (58s)
INFO 06-21 21:01:58 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 06-21 21:01:58 [model_runner.py:1171] Starting to load model openbmb/MiniCPM-V-2_6...
ERROR 06-21 21:02:00 [engine.py:458] CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 10.91 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 10.78 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 1.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 06-21 21:02:00 [engine.py:458] Traceback (most recent call last):
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 446, in run_mp_engine
ERROR 06-21 21:02:00 [engine.py:458]     engine = MQLLMEngine.from_vllm_config(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 133, in from_vllm_config
ERROR 06-21 21:02:00 [engine.py:458]     return cls(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 87, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.engine = LLMEngine(*args, **kwargs)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 265, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.model_executor = executor_class(vllm_config=vllm_config)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 53, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self._init_executor()
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
ERROR 06-21 21:02:00 [engine.py:458]     self.collective_rpc("load_model")
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
ERROR 06-21 21:02:00 [engine.py:458]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py", line 2671, in run_method
ERROR 06-21 21:02:00 [engine.py:458]     return func(*args, **kwargs)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py", line 210, in load_model
ERROR 06-21 21:02:00 [engine.py:458]     self.model_runner.load_model()
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1174, in load_model
ERROR 06-21 21:02:00 [engine.py:458]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 59, in get_model
ERROR 06-21 21:02:00 [engine.py:458]     return loader.load_model(vllm_config=vllm_config,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
ERROR 06-21 21:02:00 [engine.py:458]     model = initialize_model(vllm_config=vllm_config,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py", line 62, in initialize_model
ERROR 06-21 21:02:00 [engine.py:458]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1287, in __new__
ERROR 06-21 21:02:00 [engine.py:458]     return instance_cls(vllm_config=vllm_config, prefix=prefix)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1171, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     super().__init__(vllm_config=vllm_config, prefix=prefix)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 741, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.llm = self.init_llm(vllm_config=vllm_config,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1179, in init_llm
ERROR 06-21 21:02:00 [engine.py:458]     return Qwen2ForCausalLM(vllm_config=vllm_config, prefix=prefix)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 447, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.model = Qwen2Model(vllm_config=vllm_config,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 152, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 316, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
ERROR 06-21 21:02:00 [engine.py:458]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 627, in <listcomp>
ERROR 06-21 21:02:00 [engine.py:458]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 318, in <lambda>
ERROR 06-21 21:02:00 [engine.py:458]     lambda prefix: decoder_layer_type(config=config,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 228, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.mlp = Qwen2MLP(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 71, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.gate_up_proj = MergedColumnParallelLinear(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 547, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     super().__init__(input_size=input_size,
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 411, in __init__
ERROR 06-21 21:02:00 [engine.py:458]     self.quant_method.create_weights(
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 190, in create_weights
ERROR 06-21 21:02:00 [engine.py:458]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 06-21 21:02:00 [engine.py:458]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 06-21 21:02:00 [engine.py:458]     return func(*args, **kwargs)
ERROR 06-21 21:02:00 [engine.py:458] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 10.91 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 10.78 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 1.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 460, in run_mp_engine
    raise e from None
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 446, in run_mp_engine
    engine = MQLLMEngine.from_vllm_config(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 133, in from_vllm_config
    return cls(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 87, in __init__
    self.engine = LLMEngine(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 265, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
    self.collective_rpc("load_model")
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py", line 2671, in run_method
    return func(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py", line 210, in load_model
    self.model_runner.load_model()
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1174, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 59, in get_model
    return loader.load_model(vllm_config=vllm_config,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
    model = initialize_model(vllm_config=vllm_config,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py", line 62, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1287, in __new__
    return instance_cls(vllm_config=vllm_config, prefix=prefix)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1171, in __init__
    super().__init__(vllm_config=vllm_config, prefix=prefix)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 741, in __init__
    self.llm = self.init_llm(vllm_config=vllm_config,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py", line 1179, in init_llm
    return Qwen2ForCausalLM(vllm_config=vllm_config, prefix=prefix)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 447, in __init__
    self.model = Qwen2Model(vllm_config=vllm_config,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 152, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 316, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 626, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 627, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 318, in <lambda>
    lambda prefix: decoder_layer_type(config=config,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 228, in __init__
    self.mlp = Qwen2MLP(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 71, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 547, in __init__
    super().__init__(input_size=input_size,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 411, in __init__
    self.quant_method.create_weights(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py", line 190, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 10.91 GiB of which 136.06 MiB is free. Including non-PyTorch memory, this process has 10.78 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 1.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  ... waiting (60s)
[rank0]:[W621 21:02:01.014509819 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
  ... waiting (62s)
  ... waiting (64s)
  ... waiting (66s)
Traceback (most recent call last):
  File "/home/wiss/zhang/anaconda3/envs/vllm/bin/vllm", line 8, in <module>
    sys.exit(main())
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py", line 59, in main
    args.dispatch_function(args)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py", line 58, in cmd
    uvloop.run(run_server(args))
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 82, in run
    return loop.run_until_complete(wrapper())
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1323, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1343, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 155, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 288, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
  ... waiting (68s)
  ... waiting (70s)
  ... waiting (72s)
  ... waiting (74s)
  ... waiting (76s)
  ... waiting (78s)
  ... waiting (80s)
  ... waiting (82s)
  ... waiting (84s)
  ... waiting (86s)
  ... waiting (88s)
  ... waiting (90s)
  ... waiting (92s)
  ... waiting (94s)
  ... waiting (96s)
  ... waiting (98s)
  ... waiting (100s)
  ... waiting (102s)
  ... waiting (104s)
  ... waiting (106s)
  ... waiting (108s)
  ... waiting (110s)
  ... waiting (112s)
  ... waiting (114s)
  ... waiting (116s)
  ... waiting (118s)
  ... waiting (120s)
  ... waiting (122s)
  ... waiting (124s)
  ... waiting (126s)
  ... waiting (128s)
  ... waiting (130s)
  ... waiting (132s)
  ... waiting (134s)
  ... waiting (136s)
  ... waiting (138s)
  ... waiting (140s)
  ... waiting (142s)
  ... waiting (144s)
  ... waiting (146s)
  ... waiting (148s)
  ... waiting (150s)
  ... waiting (152s)
  ... waiting (154s)
  ... waiting (156s)
  ... waiting (158s)
  ... waiting (160s)
  ... waiting (162s)
  ... waiting (164s)
  ... waiting (166s)
  ... waiting (168s)
  ... waiting (170s)
  ... waiting (172s)
  ... waiting (174s)
  ... waiting (176s)
  ... waiting (178s)
  ... waiting (180s)
  ... waiting (182s)
  ... waiting (184s)
  ... waiting (186s)
  ... waiting (188s)
  ... waiting (190s)
  ... waiting (192s)
  ... waiting (194s)
  ... waiting (196s)
  ... waiting (198s)
  ... waiting (200s)
  ... waiting (202s)
  ... waiting (204s)
  ... waiting (206s)
  ... waiting (208s)
  ... waiting (210s)
  ... waiting (212s)
  ... waiting (214s)
  ... waiting (216s)
  ... waiting (218s)
  ... waiting (220s)
  ... waiting (222s)
  ... waiting (224s)
  ... waiting (226s)
  ... waiting (228s)
  ... waiting (230s)
  ... waiting (232s)
  ... waiting (234s)
  ... waiting (236s)
  ... waiting (238s)
  ... waiting (240s)
  ... waiting (242s)
  ... waiting (244s)
  ... waiting (246s)
  ... waiting (248s)
  ... waiting (250s)
  ... waiting (252s)
  ... waiting (254s)
  ... waiting (256s)
  ... waiting (258s)
  ... waiting (260s)
  ... waiting (262s)
  ... waiting (264s)
  ... waiting (266s)
  ... waiting (268s)
  ... waiting (270s)
  ... waiting (272s)
  ... waiting (274s)
  ... waiting (276s)
  ... waiting (278s)
  ... waiting (280s)
  ... waiting (282s)
  ... waiting (284s)
  ... waiting (286s)
  ... waiting (288s)
  ... waiting (290s)
  ... waiting (292s)
  ... waiting (294s)
  ... waiting (296s)
  ... waiting (298s)
  ... waiting (300s)
  ... waiting (302s)
  ... waiting (304s)
  ... waiting (306s)
  ... waiting (308s)
  ... waiting (310s)
  ... waiting (312s)
  ... waiting (314s)
  ... waiting (316s)
  ... waiting (318s)
  ... waiting (320s)
  ... waiting (322s)
  ... waiting (324s)
  ... waiting (326s)
  ... waiting (328s)
  ... waiting (330s)
  ... waiting (332s)
  ... waiting (334s)
  ... waiting (336s)
  ... waiting (338s)
  ... waiting (340s)
  ... waiting (342s)
  ... waiting (344s)
  ... waiting (346s)
  ... waiting (348s)
  ... waiting (350s)
  ... waiting (352s)
  ... waiting (354s)
  ... waiting (356s)
  ... waiting (358s)
  ... waiting (360s)
  ... waiting (362s)
  ... waiting (364s)
  ... waiting (366s)
  ... waiting (368s)
  ... waiting (370s)
  ... waiting (372s)
  ... waiting (374s)
  ... waiting (376s)
  ... waiting (378s)
  ... waiting (380s)
  ... waiting (382s)
  ... waiting (384s)
  ... waiting (386s)
  ... waiting (388s)
  ... waiting (390s)
  ... waiting (392s)
  ... waiting (394s)
  ... waiting (396s)
  ... waiting (398s)
  ... waiting (400s)
  ... waiting (402s)
  ... waiting (404s)
  ... waiting (406s)
  ... waiting (408s)
  ... waiting (410s)
  ... waiting (412s)
  ... waiting (414s)
  ... waiting (416s)
  ... waiting (418s)
  ... waiting (420s)
  ... waiting (422s)
  ... waiting (424s)
  ... waiting (426s)
  ... waiting (428s)
  ... waiting (430s)
  ... waiting (432s)
  ... waiting (434s)
  ... waiting (436s)
  ... waiting (438s)
  ... waiting (440s)
  ... waiting (442s)
  ... waiting (444s)
  ... waiting (446s)
  ... waiting (448s)
  ... waiting (450s)
  ... waiting (452s)
  ... waiting (454s)
  ... waiting (456s)
  ... waiting (458s)
  ... waiting (460s)
  ... waiting (462s)
  ... waiting (464s)
  ... waiting (466s)
  ... waiting (468s)
  ... waiting (470s)
  ... waiting (472s)
  ... waiting (474s)
  ... waiting (476s)
  ... waiting (478s)
  ... waiting (480s)
  ... waiting (482s)
  ... waiting (484s)
  ... waiting (486s)
  ... waiting (488s)
  ... waiting (490s)
  ... waiting (492s)
  ... waiting (494s)
  ... waiting (496s)
  ... waiting (498s)
  ... waiting (500s)
  ... waiting (502s)
  ... waiting (504s)
  ... waiting (506s)
  ... waiting (508s)
  ... waiting (510s)
  ... waiting (512s)
  ... waiting (514s)
  ... waiting (516s)
  ... waiting (518s)
  ... waiting (520s)
  ... waiting (522s)
  ... waiting (524s)
  ... waiting (526s)
  ... waiting (528s)
  ... waiting (530s)
  ... waiting (532s)
  ... waiting (534s)
  ... waiting (536s)
  ... waiting (538s)
  ... waiting (540s)
  ... waiting (542s)
  ... waiting (544s)
  ... waiting (546s)
  ... waiting (548s)
  ... waiting (550s)
  ... waiting (552s)
  ... waiting (554s)
  ... waiting (556s)
  ... waiting (558s)
  ... waiting (560s)
  ... waiting (562s)
  ... waiting (564s)
  ... waiting (566s)
  ... waiting (568s)
  ... waiting (570s)
  ... waiting (572s)
  ... waiting (574s)
  ... waiting (576s)
  ... waiting (578s)
  ... waiting (580s)
  ... waiting (582s)
  ... waiting (584s)
  ... waiting (586s)
  ... waiting (588s)
  ... waiting (590s)
  ... waiting (592s)
  ... waiting (594s)
  ... waiting (596s)
  ... waiting (598s)
  ... waiting (600s)
[ERROR] ❌ Timeout: minicpm server failed to start.
