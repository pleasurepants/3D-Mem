=== JOB START ===
Sat Jun 21 10:49:59 PM CEST 2025
worker-3
Sat Jun 21 22:49:59 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        Off | 00000000:3F:00.0 Off |                  N/A |
| 34%   42C    P8              21W / 350W |      0MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3090        Off | 00000000:40:00.0 Off |                  N/A |
| 45%   44C    P8              32W / 350W |      0MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
SLURM_JOB_ID: 75116
[INFO] CUDA_VISIBLE_DEVICES=2,3
[INFO] Starting vLLM (internvl) server on GPU 0...
[INFO] Waiting for vLLM (internvl) server to be ready...
  ... waiting (2s)
  ... waiting (4s)
  ... waiting (6s)
  ... waiting (8s)
  ... waiting (10s)
INFO 06-21 22:50:07 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (12s)
  ... waiting (14s)
  ... waiting (16s)
INFO 06-21 22:50:15 [api_server.py:1287] vLLM API server version 0.9.1
  ... waiting (18s)
INFO 06-21 22:50:15 [cli_args.py:309] non-default args: {'model': 'OpenGVLab/InternVL3-9B', 'trust_remote_code': True, 'served_model_name': ['internvl'], 'limit_mm_per_prompt': {'image': 20}}
INFO 06-21 22:50:16 [config.py:224] Replacing legacy 'type' key with 'rope_type'
  ... waiting (20s)
  ... waiting (22s)
  ... waiting (24s)
  ... waiting (26s)
  ... waiting (28s)
INFO 06-21 22:50:26 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'score', 'reward'}. Defaulting to 'generate'.
INFO 06-21 22:50:26 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.
  ... waiting (30s)
WARNING 06-21 22:50:28 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  ... waiting (32s)
WARNING 06-21 22:50:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
  ... waiting (34s)
INFO 06-21 22:50:33 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (36s)
  ... waiting (38s)
INFO 06-21 22:50:36 [core.py:455] Waiting for init message from front-end.
INFO 06-21 22:50:36 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='OpenGVLab/InternVL3-9B', speculative_config=None, tokenizer='OpenGVLab/InternVL3-9B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=internvl, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
  ... waiting (40s)
  ... waiting (42s)
WARNING 06-21 22:50:41 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x783d0fab5000>
  ... waiting (44s)
INFO 06-21 22:50:42 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 06-21 22:50:42 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 06-21 22:50:43 [registry.py:175] InternVLProcessor did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.
WARNING 06-21 22:50:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-21 22:50:43 [gpu_model_runner.py:1595] Starting to load model OpenGVLab/InternVL3-9B...
  ... waiting (46s)
INFO 06-21 22:50:43 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 06-21 22:50:44 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 06-21 22:50:45 [weight_utils.py:292] Using model weights format ['*.safetensors']
  ... waiting (48s)
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
  ... waiting (50s)
  ... waiting (52s)
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.68s/it]
  ... waiting (54s)
  ... waiting (56s)
  ... waiting (58s)
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.99s/it]
  ... waiting (60s)
  ... waiting (62s)
  ... waiting (64s)
  ... waiting (66s)
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:18<00:06,  6.42s/it]
  ... waiting (68s)
  ... waiting (70s)
  ... waiting (72s)
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.59s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.34s/it]

INFO 06-21 22:51:11 [default_loader.py:272] Loading weights took 25.48 seconds
INFO 06-21 22:51:11 [gpu_model_runner.py:1624] Model loading took 17.0389 GiB and 27.374570 seconds
INFO 06-21 22:51:11 [gpu_model_runner.py:1978] Encoder cache will be initialized with a budget of 3328 tokens, and profiled with 1 image items of the maximum feature size.
  ... waiting (74s)
  ... waiting (76s)
  ... waiting (78s)
  ... waiting (80s)
  ... waiting (82s)
  ... waiting (84s)
  ... waiting (86s)
  ... waiting (88s)
  ... waiting (90s)
  ... waiting (92s)
  ... waiting (94s)
  ... waiting (96s)
  ... waiting (98s)
  ... waiting (100s)
  ... waiting (102s)
  ... waiting (104s)
  ... waiting (106s)
  ... waiting (108s)
  ... waiting (110s)
INFO 06-21 22:51:49 [backends.py:462] Using cache directory: /home/wiss/zhang/.cache/vllm/torch_compile_cache/dbbbe78f81/rank_0_0 for vLLM's torch.compile
INFO 06-21 22:51:49 [backends.py:472] Dynamo bytecode transform time: 32.02 s
  ... waiting (112s)
  ... waiting (114s)
  ... waiting (116s)
  ... waiting (118s)
INFO 06-21 22:51:56 [backends.py:161] Cache the graph of shape None for later use
  ... waiting (120s)
  ... waiting (122s)
  ... waiting (124s)
  ... waiting (126s)
  ... waiting (128s)
  ... waiting (130s)
  ... waiting (132s)
  ... waiting (134s)
  ... waiting (136s)
  ... waiting (138s)
  ... waiting (140s)
  ... waiting (142s)
  ... waiting (144s)
  ... waiting (146s)
  ... waiting (148s)
  ... waiting (150s)
  ... waiting (152s)
  ... waiting (154s)
  ... waiting (156s)
  ... waiting (158s)
  ... waiting (160s)
INFO 06-21 22:52:39 [backends.py:173] Compiling a graph for general shape takes 49.25 s
  ... waiting (162s)
  ... waiting (164s)
  ... waiting (166s)
  ... waiting (168s)
  ... waiting (170s)
  ... waiting (172s)
  ... waiting (174s)
  ... waiting (176s)
  ... waiting (178s)
INFO 06-21 22:52:58 [monitor.py:34] torch.compile takes 81.28 s in total
  ... waiting (180s)
INFO 06-21 22:53:01 [gpu_worker.py:227] Available KV cache memory: 3.01 GiB
  ... waiting (182s)
INFO 06-21 22:53:01 [kv_cache_utils.py:715] GPU KV cache size: 65,696 tokens
INFO 06-21 22:53:01 [kv_cache_utils.py:719] Maximum concurrency for 8,192 tokens per request: 8.02x
  ... waiting (184s)
  ... waiting (186s)
  ... waiting (188s)
  ... waiting (190s)
  ... waiting (192s)
  ... waiting (194s)
  ... waiting (196s)
  ... waiting (198s)
  ... waiting (200s)
  ... waiting (202s)
  ... waiting (204s)
  ... waiting (206s)
  ... waiting (208s)
  ... waiting (210s)
  ... waiting (212s)
  ... waiting (214s)
INFO 06-21 22:53:34 [gpu_model_runner.py:2048] Graph capturing finished in 33 secs, took 2.50 GiB
ERROR 06-21 22:53:34 [core.py:515] EngineCore failed to start.
ERROR 06-21 22:53:34 [core.py:515] Traceback (most recent call last):
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1902, in _dummy_sampler_run
ERROR 06-21 22:53:34 [core.py:515]     sampler_output = self.sampler(logits=logits,
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 06-21 22:53:34 [core.py:515]     return self._call_impl(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 06-21 22:53:34 [core.py:515]     return forward_call(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 52, in forward
ERROR 06-21 22:53:34 [core.py:515]     sampled = self.sample(logits, sampling_metadata)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 118, in sample
ERROR 06-21 22:53:34 [core.py:515]     random_sampled = self.topk_topp_sampler(
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 06-21 22:53:34 [core.py:515]     return self._call_impl(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 06-21 22:53:34 [core.py:515]     return forward_call(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 81, in forward_native
ERROR 06-21 22:53:34 [core.py:515]     logits = apply_top_k_top_p(logits, k, p)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 184, in apply_top_k_top_p
ERROR 06-21 22:53:34 [core.py:515]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
ERROR 06-21 22:53:34 [core.py:515] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 233.69 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, with 79.88 MiB allocated in private pools (e.g., CUDA Graphs), and 38.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 06-21 22:53:34 [core.py:515] 
ERROR 06-21 22:53:34 [core.py:515] The above exception was the direct cause of the following exception:
ERROR 06-21 22:53:34 [core.py:515] 
ERROR 06-21 22:53:34 [core.py:515] Traceback (most recent call last):
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 506, in run_engine_core
ERROR 06-21 22:53:34 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 06-21 22:53:34 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 83, in __init__
ERROR 06-21 22:53:34 [core.py:515]     self._initialize_kv_caches(vllm_config)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 168, in _initialize_kv_caches
ERROR 06-21 22:53:34 [core.py:515]     self.model_executor.initialize_from_config(kv_cache_configs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 66, in initialize_from_config
ERROR 06-21 22:53:34 [core.py:515]     self.collective_rpc("compile_or_warm_up_model")
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
ERROR 06-21 22:53:34 [core.py:515]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py", line 2671, in run_method
ERROR 06-21 22:53:34 [core.py:515]     return func(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 271, in compile_or_warm_up_model
ERROR 06-21 22:53:34 [core.py:515]     self.model_runner._dummy_sampler_run(
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 06-21 22:53:34 [core.py:515]     return func(*args, **kwargs)
ERROR 06-21 22:53:34 [core.py:515]   File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1906, in _dummy_sampler_run
ERROR 06-21 22:53:34 [core.py:515]     raise RuntimeError(
ERROR 06-21 22:53:34 [core.py:515] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1902, in _dummy_sampler_run
    sampler_output = self.sampler(logits=logits,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 52, in forward
    sampled = self.sample(logits, sampling_metadata)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 118, in sample
    random_sampled = self.topk_topp_sampler(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 81, in forward_native
    logits = apply_top_k_top_p(logits, k, p)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 184, in apply_top_k_top_p
    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 233.69 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, with 79.88 MiB allocated in private pools (e.g., CUDA Graphs), and 38.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 519, in run_engine_core
    raise e
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 506, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 83, in __init__
    self._initialize_kv_caches(vllm_config)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 168, in _initialize_kv_caches
    self.model_executor.initialize_from_config(kv_cache_configs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 66, in initialize_from_config
    self.collective_rpc("compile_or_warm_up_model")
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/utils.py", line 2671, in run_method
    return func(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 271, in compile_or_warm_up_model
    self.model_runner._dummy_sampler_run(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1906, in _dummy_sampler_run
    raise RuntimeError(
RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
  ... waiting (216s)
[rank0]:[W621 22:53:35.510236069 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
  ... waiting (218s)
Traceback (most recent call last):
  File "/home/wiss/zhang/anaconda3/envs/vllm/bin/vllm", line 8, in <module>
    sys.exit(main())
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py", line 59, in main
    args.dispatch_function(args)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py", line 58, in cmd
    uvloop.run(run_server(args))
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 82, in run
    return loop.run_until_complete(wrapper())
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1323, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1343, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 155, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 191, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 162, in from_vllm_config
    return cls(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 124, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 93, in make_async_mp_client
    return AsyncMPClient(vllm_config, executor_class, log_stats,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 716, in __init__
    super().__init__(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 422, in __init__
    self._init_engines_direct(vllm_config, local_only,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 491, in _init_engines_direct
    self._wait_for_engine_startup(handshake_socket, input_address,
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 511, in _wait_for_engine_startup
    wait_for_engine_startup(
  File "/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/v1/utils.py", line 494, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
  ... waiting (220s)
  ... waiting (222s)
  ... waiting (224s)
  ... waiting (226s)
  ... waiting (228s)
  ... waiting (230s)
  ... waiting (232s)
  ... waiting (234s)
  ... waiting (236s)
  ... waiting (238s)
  ... waiting (240s)
  ... waiting (242s)
  ... waiting (244s)
  ... waiting (246s)
  ... waiting (248s)
  ... waiting (250s)
  ... waiting (252s)
  ... waiting (254s)
  ... waiting (256s)
  ... waiting (258s)
  ... waiting (260s)
  ... waiting (262s)
  ... waiting (264s)
  ... waiting (266s)
  ... waiting (268s)
  ... waiting (270s)
  ... waiting (272s)
  ... waiting (274s)
  ... waiting (276s)
  ... waiting (278s)
  ... waiting (280s)
  ... waiting (282s)
  ... waiting (284s)
  ... waiting (286s)
  ... waiting (288s)
  ... waiting (290s)
  ... waiting (292s)
  ... waiting (294s)
  ... waiting (296s)
  ... waiting (298s)
  ... waiting (300s)
  ... waiting (302s)
  ... waiting (304s)
  ... waiting (306s)
  ... waiting (308s)
  ... waiting (310s)
  ... waiting (312s)
  ... waiting (314s)
  ... waiting (316s)
  ... waiting (318s)
  ... waiting (320s)
  ... waiting (322s)
  ... waiting (324s)
  ... waiting (326s)
  ... waiting (328s)
  ... waiting (330s)
  ... waiting (332s)
  ... waiting (334s)
  ... waiting (336s)
  ... waiting (338s)
  ... waiting (340s)
  ... waiting (342s)
  ... waiting (344s)
  ... waiting (346s)
  ... waiting (348s)
  ... waiting (350s)
  ... waiting (352s)
  ... waiting (354s)
  ... waiting (356s)
  ... waiting (358s)
  ... waiting (360s)
  ... waiting (362s)
  ... waiting (364s)
  ... waiting (366s)
  ... waiting (368s)
  ... waiting (370s)
  ... waiting (372s)
  ... waiting (374s)
  ... waiting (376s)
  ... waiting (378s)
  ... waiting (380s)
  ... waiting (382s)
  ... waiting (384s)
  ... waiting (386s)
  ... waiting (388s)
  ... waiting (390s)
  ... waiting (392s)
  ... waiting (394s)
  ... waiting (396s)
  ... waiting (398s)
  ... waiting (400s)
  ... waiting (402s)
  ... waiting (404s)
  ... waiting (406s)
  ... waiting (408s)
  ... waiting (410s)
  ... waiting (412s)
  ... waiting (414s)
  ... waiting (416s)
  ... waiting (418s)
  ... waiting (420s)
  ... waiting (422s)
  ... waiting (424s)
  ... waiting (426s)
  ... waiting (428s)
  ... waiting (430s)
  ... waiting (432s)
  ... waiting (434s)
  ... waiting (436s)
  ... waiting (438s)
  ... waiting (440s)
  ... waiting (442s)
  ... waiting (444s)
  ... waiting (446s)
  ... waiting (448s)
  ... waiting (450s)
  ... waiting (452s)
  ... waiting (454s)
  ... waiting (456s)
  ... waiting (458s)
  ... waiting (460s)
  ... waiting (462s)
  ... waiting (464s)
  ... waiting (466s)
  ... waiting (468s)
  ... waiting (470s)
  ... waiting (472s)
  ... waiting (474s)
  ... waiting (476s)
  ... waiting (478s)
  ... waiting (480s)
  ... waiting (482s)
  ... waiting (484s)
  ... waiting (486s)
  ... waiting (488s)
  ... waiting (490s)
  ... waiting (492s)
  ... waiting (494s)
  ... waiting (496s)
  ... waiting (498s)
  ... waiting (500s)
  ... waiting (502s)
  ... waiting (504s)
  ... waiting (506s)
  ... waiting (508s)
  ... waiting (510s)
  ... waiting (512s)
  ... waiting (514s)
  ... waiting (516s)
  ... waiting (518s)
  ... waiting (520s)
  ... waiting (522s)
  ... waiting (524s)
  ... waiting (526s)
  ... waiting (528s)
  ... waiting (530s)
  ... waiting (532s)
  ... waiting (534s)
  ... waiting (536s)
  ... waiting (538s)
  ... waiting (540s)
  ... waiting (542s)
  ... waiting (544s)
  ... waiting (546s)
  ... waiting (548s)
  ... waiting (550s)
  ... waiting (552s)
  ... waiting (554s)
  ... waiting (556s)
  ... waiting (558s)
  ... waiting (560s)
  ... waiting (562s)
  ... waiting (564s)
  ... waiting (566s)
  ... waiting (568s)
  ... waiting (570s)
  ... waiting (572s)
  ... waiting (574s)
  ... waiting (576s)
  ... waiting (578s)
  ... waiting (580s)
  ... waiting (582s)
  ... waiting (584s)
  ... waiting (586s)
  ... waiting (588s)
  ... waiting (590s)
  ... waiting (592s)
  ... waiting (594s)
  ... waiting (596s)
  ... waiting (598s)
  ... waiting (600s)
[ERROR] ❌ Timeout: internvl server failed to start.
