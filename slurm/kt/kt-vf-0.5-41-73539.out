Wed Jun 11 11:28:05 PM CEST 2025
worker-2
/home/wiss/zhang/anaconda3/envs/3dmem/bin/python
Running on MASTER_NODE=worker-2, MASTER_PORT=8744, RDZV_ID=4017
2.3.0
00:00:00 - Downloading `prism-dinosiglip+7b from HF Hub
00:00:00 - Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-384px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`/home/wiss/zhang/.cache/huggingface/hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip+7b/checkpoints/latest-checkpoint.pt`[/]
00:00:00 - Loading Vision Backbone [bold]dinosiglip-vit-so-384px[/]
00:00:04 - Loading pretrained weights from Hugging Face hub (timm/vit_large_patch14_reg4_dinov2.lvd142m)
00:00:04 - [timm/vit_large_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
00:00:04 - Resized position embedding: (37, 37) to (27, 27).
00:00:09 - Loading pretrained weights from Hugging Face hub (('timm/ViT-SO400M-14-SigLIP-384', 'open_clip_pytorch_model.bin'))
00:00:09 - [timm/ViT-SO400M-14-SigLIP-384] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
00:00:10 - Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
00:00:10 - Building empty [bold]llama2[/] LLM from [underline]`meta-llama/Llama-2-7b-hf`[/]
00:01:26 - Loading [bold]llama2[/] (Fast) Tokenizer via the AutoTokenizer API
00:01:29 - Loading VLM [bold blue]prism-dinosiglip+7b[/] from Checkpoint; Freezing Weights ðŸ¥¶
00:01:47 - Loaded VLM in 106.987s
00:01:47 - ***** Running exp_eval_aeqa *****
Loading snapshot prismatic model
prismatic loaded.
Loading model from llava-hf/llava-1.5-7b-hf ...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.89s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.69s/it]
00:01:54 - Total number of questions: 41
00:01:54 - number of questions after splitting: 41
00:01:54 - question path: data/aeqa_questions-41.json
00:01:54 - Load YOLO model yolov8x-world.pt successful!
00:01:57 - Load SAM model sam_l.pt successful!
00:01:57 - Loaded ViT-B-32 model config.
00:01:58 - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
00:01:59 - Load CLIP model successful!
00:01:59 - Question 00c2be2a-1377-4fae-a889-30936b7890c3 already processed
00:01:59 - Question 013bb857-f47d-4b50-add4-023cc4ff414c already processed
00:01:59 - Question 01fcc568-f51e-4e12-b976-5dc8d554135a already processed
00:01:59 - Question 0df60236-15ad-4166-a31a-a98d14214fdb already processed
00:01:59 - Question 109eaad4-6e68-4da1-8f98-a0d8589ec26d already processed
00:01:59 - Question 1b36e675-74ff-46ad-8caa-c33da46a5a67 already processed
00:01:59 - Question 1dcdd225-eba2-4ba1-97b6-c4cdc7ca4e9b already processed
00:01:59 - Question 30dc765d-80c3-4901-9c69-65e6b48e254a already processed
00:01:59 - Question 3a5be057-47d2-4f78-98a9-729ef19b3d8b already processed
00:01:59 - Question 45a5e082-a9e9-47ca-a036-dfafba92b16c already processed
00:01:59 - Question 48d8aa7f-61cb-469b-9b6d-2549d1210281 already processed
00:01:59 - Question 4cc4212e-0db2-421f-8bb5-93817e58f9b4 already processed
00:01:59 - Question 4dbd213e-56cd-481a-8ff5-ed9a8d636dbc already processed
00:01:59 - Question 67cd7145-4b1f-4b2a-a698-8e4738cb7c41 already processed
00:01:59 - Question 6852b358-4820-471d-9263-d32ef0cecd0b already processed
00:01:59 - Question 6d132959-fd48-4fef-a736-4e5853849547 already processed
00:01:59 - Question 7ebac357-a338-4ce0-975a-62141e90a3c3 already processed
00:01:59 - Question 90ab6389-d85e-42ad-b44a-af4849da2631 already processed
00:01:59 - 
========
Index: 18 Scene: 00835-q3zU7Yy5E5s
00:02:01 - Loaded 192 classes from scannet 200: data/scannet200_classes.txt!!!
00:02:01 - Load scene 00835-q3zU7Yy5E5s successfully with semantic texture
00:02:06 - 

Question id 911693d9-2d28-4ff2-83a9-c67b83753831 initialization successful!
00:02:06 - 
== step: 0
00:02:08 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.02 seconds
00:02:13 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.01 seconds
00:02:14 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.01 seconds
00:02:16 - Step 0, update snapshots, 2 objects, 2 snapshots
00:02:18 - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:02:18 - Prefiltering selected classes: ['bucket']
00:02:18 - Prefiltering snapshot: 2 -> 1
00:02:23 - Input prompt:
00:02:23 - Task: You are an agent in an indoor scene tasked with answering questions by observing the surroundings and exploring the environment. To answer the question, you are required to choose either a snapshot as the answer or a Frontier to further explore.
Definitions:
snapshot: A focused observation of several objects. Choosing a snapshot means that this snapshot image contains enough information for you to answer the question. If you choose a snapshot, you need to directly give an answer to the question. If you don't have enough information to give an answer, then don't choose a snapshot.
Frontier: An observation of an unexplored region that could potentially lead to new information for answering the question. Selecting a frontier means that you will further explore that direction. If you choose a Frontier, you need to explain why you would like to choose that direction to explore.
Question: What can be seen through the window in the living room?
Select the Frontier/snapshot that would help find the answer of the question.
The following is the egocentric view of the agent in forward direction: [iVBORw0KGg...]
The followings are all the snapshots that you can choose (followed with contained object classes)
Please note that the contained classes may not be accurate (wrong classes/missing classes) due to the limitation of the object detection model. So you still need to utilize the images to make decisions.
snapshot 0 [iVBORw0KGg...]bucket
The followings are all the Frontiers that you can explore: 
Frontier 0 [iVBORw0KGg...]
Please provide your answer in the following format: 'snapshot i
[Answer]' or 'Frontier i
[Reason]', where i is the index of the snapshot or frontier you choose. For example, if you choose the first snapshot, you can return 'snapshot 0
The fruit bowl is on the kitchen counter.'. If you choose the second frontier, you can return 'Frontier 1
I see a door that may lead to the living room.'.
Note that if you choose a snapshot to answer the question, (1) you should give a direct answer that can be understood by others. Don't mention words like 'snapshot', 'on the left of the image', etc; (2) you can also utilize other snapshots, frontiers and egocentric views to gather more information, but you should always choose one most relevant snapshot to answer the question.

llava-1.5-7b-hf loaded.
Only one frontier provided: automatically selected as winner.
Traceback (most recent call last):
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_kt.py", line 456, in <module>
    main(llava_pair_selector, vlm_pred, cfg, args.start_ratio, args.end_ratio)
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_kt.py", line 276, in main
    vlm_response = query_vlm_for_response(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src_kt/query_vlm_aeqa.py", line 49, in query_vlm_for_response
    outputs, snapshot_id_mapping, reason, n_filtered_snapshots = explore_step(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src_kt/eval_utils_gpt_aeqa.py", line 952, in explore_step
    frontier_index, reason, top_indices = pairwise_voting_frontier_list(frontier_imgs, question, llava_pairwise_selector)
ValueError: not enough values to unpack (expected 3, got 2)
