=== JOB START ===
Sun Jun 22 05:08:10 PM CEST 2025
worker-7
Sun Jun 22 17:08:10 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:05:00.0 Off |                    0 |
|  0%   29C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:06:00.0 Off |                    0 |
|  0%   30C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:45:00.0 Off |                    0 |
|  0%   30C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   38C    P0              76W / 300W |  42380MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    3   N/A  N/A    341397      C   ...conda3/envs/llamafactory/bin/python    42372MiB |
+---------------------------------------------------------------------------------------+
SLURM_JOB_ID: 75139
[INFO] CUDA_VISIBLE_DEVICES=0,1
[INFO] Starting vLLM (qwen) server on GPU 0...
[INFO] Waiting for vLLM (qwen) server to be ready...
  ... waiting (2s)
  ... waiting (4s)
  ... waiting (6s)
  ... waiting (8s)
INFO 06-22 17:08:19 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (10s)
  ... waiting (12s)
  ... waiting (14s)
  ... waiting (16s)
  ... waiting (18s)
INFO 06-22 17:08:28 [api_server.py:1287] vLLM API server version 0.9.1
INFO 06-22 17:08:29 [cli_args.py:309] non-default args: {'model': 'Qwen/Qwen2-VL-7B-Instruct', 'served_model_name': ['qwen'], 'limit_mm_per_prompt': {'image': 50}}
  ... waiting (20s)
  ... waiting (22s)
  ... waiting (24s)
  ... waiting (26s)
  ... waiting (28s)
  ... waiting (30s)
  ... waiting (32s)
INFO 06-22 17:08:42 [config.py:823] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.
INFO 06-22 17:08:42 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.
  ... waiting (34s)
WARNING 06-22 17:08:45 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
  ... waiting (36s)
  ... waiting (38s)
INFO 06-22 17:08:48 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (40s)
  ... waiting (42s)
INFO 06-22 17:08:53 [core.py:455] Waiting for init message from front-end.
INFO 06-22 17:08:53 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2-VL-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
  ... waiting (44s)
WARNING 06-22 17:08:54 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x765f87da4520>
INFO 06-22 17:08:54 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
  ... waiting (46s)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
  ... waiting (48s)
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
  ... waiting (50s)
  ... waiting (52s)
Unused or unrecognized kwargs: return_tensors.
  ... waiting (54s)
WARNING 06-22 17:09:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-22 17:09:04 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2-VL-7B-Instruct...
INFO 06-22 17:09:04 [gpu_model_runner.py:1600] Loading model from scratch...
WARNING 06-22 17:09:04 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-22 17:09:04 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 06-22 17:09:05 [weight_utils.py:292] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
  ... waiting (56s)
  ... waiting (58s)
  ... waiting (60s)
  ... waiting (62s)
Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:07<00:29,  7.32s/it]
  ... waiting (64s)
Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:12,  4.23s/it]
  ... waiting (66s)
  ... waiting (68s)
  ... waiting (70s)
Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:16<00:10,  5.47s/it]
  ... waiting (72s)
  ... waiting (74s)
  ... waiting (76s)
  ... waiting (78s)
Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:23<00:05,  5.98s/it]
  ... waiting (80s)
  ... waiting (82s)
  ... waiting (84s)
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  6.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  6.03s/it]

INFO 06-22 17:09:35 [default_loader.py:272] Loading weights took 30.22 seconds
  ... waiting (86s)
INFO 06-22 17:09:36 [gpu_model_runner.py:1624] Model loading took 15.4752 GiB and 31.145285 seconds
INFO 06-22 17:09:37 [gpu_model_runner.py:1978] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
  ... waiting (88s)
  ... waiting (90s)
  ... waiting (92s)
  ... waiting (94s)
  ... waiting (96s)
  ... waiting (98s)
  ... waiting (100s)
  ... waiting (102s)
  ... waiting (104s)
  ... waiting (106s)
  ... waiting (108s)
INFO 06-22 17:09:59 [backends.py:462] Using cache directory: /home/wiss/zhang/.cache/vllm/torch_compile_cache/19cf1104e4/rank_0_0 for vLLM's torch.compile
INFO 06-22 17:09:59 [backends.py:472] Dynamo bytecode transform time: 8.43 s
  ... waiting (110s)
  ... waiting (112s)
  ... waiting (114s)
  ... waiting (116s)
INFO 06-22 17:10:06 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 6.614 s
INFO 06-22 17:10:07 [monitor.py:34] torch.compile takes 8.43 s in total
  ... waiting (118s)
INFO 06-22 17:10:09 [gpu_worker.py:227] Available KV cache memory: 21.30 GiB
INFO 06-22 17:10:10 [kv_cache_utils.py:715] GPU KV cache size: 398,816 tokens
INFO 06-22 17:10:10 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 12.17x
  ... waiting (120s)
  ... waiting (122s)
  ... waiting (124s)
  ... waiting (126s)
  ... waiting (128s)
  ... waiting (130s)
  ... waiting (132s)
  ... waiting (134s)
  ... waiting (136s)
  ... waiting (138s)
  ... waiting (140s)
  ... waiting (142s)
  ... waiting (144s)
INFO 06-22 17:10:36 [gpu_model_runner.py:2048] Graph capturing finished in 26 secs, took 1.58 GiB
INFO 06-22 17:10:36 [core.py:171] init engine (profile, create kv cache, warmup model) took 60.19 seconds
  ... waiting (146s)
  ... waiting (148s)
INFO 06-22 17:10:38 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 24926
WARNING 06-22 17:10:38 [config.py:1363] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-22 17:10:38 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.01, 'top_k': 1, 'top_p': 0.001}
INFO 06-22 17:10:38 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.01, 'top_k': 1, 'top_p': 0.001}
INFO 06-22 17:10:38 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 06-22 17:10:38 [launcher.py:29] Available routes are:
INFO 06-22 17:10:38 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /health, Methods: GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /load, Methods: GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /ping, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /ping, Methods: GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /version, Methods: GET
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /pooling, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /classify, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /score, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /rerank, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /invocations, Methods: POST
INFO 06-22 17:10:38 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [1443087]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:40670 - "GET /v1/models HTTP/1.1" 200 OK
[INFO] ✅ qwen API is ready!
[INFO] Starting AEQA evaluation on GPU 1 (3dmem env)...
00:00:00 - ***** Running exp_eval_aeqa *****
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:00,  6.16it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 22.61it/s]
00:00:07 - Total number of questions: 41
00:00:07 - number of questions after splitting: 41
00:00:07 - question path: data/aeqa_questions-41.json
00:00:08 - Load YOLO model yolov8x-world.pt successful!
00:00:12 - Load SAM model sam_l.pt successful!
00:00:12 - Loaded ViT-B-32 model config.
00:00:13 - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
00:00:14 - Load CLIP model successful!
00:00:14 - 
========
Index: 0 Scene: 00824-Dd4bFSTQ8gi
00:00:20 - Loaded 192 classes from scannet 200: data/scannet200_classes.txt!!!
00:00:20 - Load scene 00824-Dd4bFSTQ8gi successfully with semantic texture
00:00:28 - 

Question id 00c2be2a-1377-4fae-a889-30936b7890c3 initialization successful!
00:00:28 - 
== step: 0
00:00:36 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.29 seconds
00:00:40 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.18 seconds
Traceback (most recent call last):
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_qwen.py", line 445, in <module>
    main(vllm_model, processor, cfg, args.start_ratio, args.end_ratio)
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_qwen.py", line 181, in main
    annotated_rgb, added_obj_ids, _ = scene.update_scene_graph(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src/scene_aeqa.py", line 527, in update_scene_graph
    spatial_sim = compute_spatial_similarities(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src/conceptgraph/slam/mapping.py", line 50, in compute_spatial_similarities
    spatial_sim = compute_overlap_matrix_general(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src/conceptgraph/slam/utils.py", line 643, in compute_overlap_matrix_general
    ious = compute_3d_iou_accurate_batch(bbox_a, bbox_b)  # (m, n)
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src/conceptgraph/utils/ious.py", line 174, in compute_3d_iou_accurate_batch
    import pytorch3d.ops as ops
  File "/home/wiss/zhang/anaconda3/envs/3dmem/lib/python3.9/site-packages/pytorch3d/ops/__init__.py", line 7, in <module>
    from .ball_query import ball_query
  File "/home/wiss/zhang/anaconda3/envs/3dmem/lib/python3.9/site-packages/pytorch3d/ops/ball_query.py", line 10, in <module>
    from pytorch3d import _C
ImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory
[INFO] AEQA finished. Killing vLLM server (PID=1443087)...
=== JOB END ===
INFO 06-22 17:12:05 [launcher.py:80] Shutting down FastAPI HTTP server.
