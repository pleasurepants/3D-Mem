Thu Jun 12 02:56:17 PM CEST 2025
worker-6
/home/wiss/zhang/anaconda3/envs/3dmem/bin/python
Running on MASTER_NODE=worker-6, MASTER_PORT=8813, RDZV_ID=20806
2.3.0
00:00:00 - Downloading `prism-dinosiglip+7b from HF Hub
00:00:00 - Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-384px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`/home/wiss/zhang/.cache/huggingface/hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip+7b/checkpoints/latest-checkpoint.pt`[/]
00:00:00 - Loading Vision Backbone [bold]dinosiglip-vit-so-384px[/]
00:00:05 - Loading pretrained weights from Hugging Face hub (timm/vit_large_patch14_reg4_dinov2.lvd142m)
00:00:05 - [timm/vit_large_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
00:00:05 - Resized position embedding: (37, 37) to (27, 27).
00:00:11 - Loading pretrained weights from Hugging Face hub (('timm/ViT-SO400M-14-SigLIP-384', 'open_clip_pytorch_model.bin'))
00:00:11 - [timm/ViT-SO400M-14-SigLIP-384] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
00:00:11 - Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
00:00:11 - Building empty [bold]llama2[/] LLM from [underline]`meta-llama/Llama-2-7b-hf`[/]
00:01:32 - Loading [bold]llama2[/] (Fast) Tokenizer via the AutoTokenizer API
00:01:36 - Loading VLM [bold blue]prism-dinosiglip+7b[/] from Checkpoint; Freezing Weights ðŸ¥¶
00:01:57 - Loaded VLM in 117.295s
00:01:57 - ***** Running exp_eval_aeqa *****
Loading snapshot prismatic model
prismatic loaded.
Loading model from llava-hf/llava-1.5-7b-hf ...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.76s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.56s/it]
00:02:04 - Total number of questions: 41
00:02:04 - number of questions after splitting: 41
00:02:04 - question path: data/aeqa_questions-41.json
00:02:04 - Load YOLO model yolov8x-world.pt successful!
00:02:07 - Load SAM model sam_l.pt successful!
00:02:07 - Loaded ViT-B-32 model config.
00:02:09 - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
00:02:09 - Load CLIP model successful!
00:02:09 - 
========
Index: 0 Scene: 00824-Dd4bFSTQ8gi
00:02:14 - Loaded 192 classes from scannet 200: data/scannet200_classes.txt!!!
00:02:14 - Load scene 00824-Dd4bFSTQ8gi successfully with semantic texture
00:02:19 - 

Question id 00c2be2a-1377-4fae-a889-30936b7890c3 initialization successful!
00:02:19 - 
== step: 0
00:02:20 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.07 seconds
00:02:22 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.12 seconds
00:02:24 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.08 seconds
00:02:25 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.05 seconds
00:02:26 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.09 seconds
00:02:27 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.09 seconds
00:02:28 - Done! Execution time of detections_to_obj_pcd_and_bbox function: 0.08 seconds
00:02:30 - Step 0, update snapshots, 12 objects, 4 snapshots
llava-1.5-7b-hf loaded.
Traceback (most recent call last):
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_point.py", line 456, in <module>
    main(llava_pair_selector, vlm_pred, cfg, args.start_ratio, args.end_ratio)
  File "/home/wiss/zhang/code/openeqa/3D-Mem/run_aeqa_evaluation_point.py", line 276, in main
    vlm_response = query_vlm_for_response(
  File "/home/wiss/zhang/code/openeqa/3D-Mem/src_kt/query_vlm_aeqa.py", line 49, in query_vlm_for_response
    outputs, snapshot_id_mapping, reason, n_filtered_snapshots = explore_step(
TypeError: explore_step() got multiple values for argument 'verbose'
