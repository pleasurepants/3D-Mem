=== JOB START ===
Sun Jun 22 10:51:32 PM CEST 2025
worker-2
Sun Jun 22 22:51:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Quadro RTX 8000                Off |   00000000:1B:00.0 Off |                  Off |
| 33%   42C    P3             51W /  260W |       1MiB /  49152MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Quadro RTX 8000                Off |   00000000:3D:00.0 Off |                  Off |
| 33%   42C    P3             53W /  260W |       1MiB /  49152MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
SLURM_JOB_ID: 75214
[INFO] CUDA_VISIBLE_DEVICES=1,2
[INFO] Starting vLLM (minicpm) server on GPU 0...
[INFO] Waiting for vLLM (minicpm) server to be ready...
  ... waiting (2s)
  ... waiting (4s)
  ... waiting (6s)
INFO 06-22 22:51:38 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (8s)
  ... waiting (10s)
  ... waiting (12s)
  ... waiting (14s)
INFO 06-22 22:51:46 [api_server.py:1287] vLLM API server version 0.9.1
INFO 06-22 22:51:46 [cli_args.py:309] non-default args: {'model': 'openbmb/MiniCPM-V-2_6', 'trust_remote_code': True, 'served_model_name': ['minicpm'], 'limit_mm_per_prompt': {'image': 20}}
  ... waiting (16s)
  ... waiting (18s)
  ... waiting (20s)
  ... waiting (22s)
  ... waiting (24s)
  ... waiting (26s)
INFO 06-22 22:51:57 [config.py:823] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
WARNING 06-22 22:51:57 [config.py:3220] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 06-22 22:51:57 [config.py:3271] Casting torch.bfloat16 to torch.float16.
WARNING 06-22 22:51:57 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
INFO 06-22 22:51:57 [api_server.py:265] Started engine process with PID 3256583
  ... waiting (28s)
WARNING 06-22 22:51:59 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
  ... waiting (30s)
INFO 06-22 22:52:02 [__init__.py:244] Automatically detected platform cuda.
  ... waiting (32s)
  ... waiting (34s)
INFO 06-22 22:52:05 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='openbmb/MiniCPM-V-2_6', speculative_config=None, tokenizer='openbmb/MiniCPM-V-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=minicpm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
INFO 06-22 22:52:07 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 06-22 22:52:07 [cuda.py:324] Using XFormers backend.
  ... waiting (36s)
INFO 06-22 22:52:08 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 06-22 22:52:08 [model_runner.py:1171] Starting to load model openbmb/MiniCPM-V-2_6...
INFO 06-22 22:52:08 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 06-22 22:52:08 [cuda.py:324] Using XFormers backend.
INFO 06-22 22:52:09 [weight_utils.py:292] Using model weights format ['*.safetensors']
  ... waiting (38s)
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.37it/s]
  ... waiting (40s)
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.37s/it]
  ... waiting (42s)
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.63s/it]
  ... waiting (44s)
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.56s/it]

INFO 06-22 22:52:15 [default_loader.py:272] Loading weights took 6.34 seconds
INFO 06-22 22:52:16 [model_runner.py:1203] Model loading took 15.1267 GiB and 7.512403 seconds
  ... waiting (46s)
/home/wiss/zhang/anaconda3/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:609: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
  ... waiting (48s)
  ... waiting (50s)
  ... waiting (52s)
  ... waiting (54s)
  ... waiting (56s)
  ... waiting (58s)
  ... waiting (60s)
  ... waiting (62s)
  ... waiting (64s)
  ... waiting (66s)
  ... waiting (68s)
  ... waiting (70s)
  ... waiting (72s)
  ... waiting (74s)
  ... waiting (76s)
INFO 06-22 22:52:48 [worker.py:294] Memory profiling takes 32.21 seconds
INFO 06-22 22:52:48 [worker.py:294] the current vLLM instance can use total_gpu_memory (47.45GiB) x gpu_memory_utilization (0.90) = 42.70GiB
INFO 06-22 22:52:48 [worker.py:294] model weights take 15.13GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 20.32GiB; the rest of the memory reserved for KV Cache is 7.19GiB.
INFO 06-22 22:52:48 [executor_base.py:113] # cuda blocks: 8415, # CPU blocks: 4681
INFO 06-22 22:52:48 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 4.11x
  ... waiting (78s)
  ... waiting (80s)
INFO 06-22 22:52:52 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]  ... waiting (82s)
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:19,  1.65it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:18,  1.70it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:17,  1.73it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:17,  1.73it/s]  ... waiting (84s)
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.73it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:16,  1.74it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:15,  1.75it/s]  ... waiting (86s)
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:14,  1.74it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:14,  1.73it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:13,  1.73it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:13,  1.75it/s]  ... waiting (88s)
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:12,  1.75it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:11,  1.76it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:08<00:11,  1.76it/s]  ... waiting (90s)
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:10,  1.77it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:09<00:10,  1.79it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:09,  1.80it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:10<00:08,  1.81it/s]  ... waiting (92s)
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:11<00:08,  1.81it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:07,  1.82it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:12<00:07,  1.83it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.84it/s]  ... waiting (94s)
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:13<00:05,  1.84it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.84it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:14<00:04,  1.84it/s]  ... waiting (96s)
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.86it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:15<00:03,  1.85it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.85it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:16<00:02,  1.85it/s]  ... waiting (98s)
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:17<00:02,  1.79it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:17<00:01,  1.83it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:18<00:01,  1.87it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:18<00:00,  1.89it/s]  ... waiting (100s)
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.90it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.80it/s]
INFO 06-22 22:53:11 [model_runner.py:1671] Graph capturing finished in 19 secs, took 0.21 GiB
INFO 06-22 22:53:11 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 55.65 seconds
INFO 06-22 22:53:12 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 06-22 22:53:12 [launcher.py:29] Available routes are:
INFO 06-22 22:53:12 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 06-22 22:53:12 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 06-22 22:53:12 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 06-22 22:53:12 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 06-22 22:53:12 [launcher.py:37] Route: /health, Methods: GET
INFO 06-22 22:53:12 [launcher.py:37] Route: /load, Methods: GET
INFO 06-22 22:53:12 [launcher.py:37] Route: /ping, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /ping, Methods: GET
INFO 06-22 22:53:12 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 06-22 22:53:12 [launcher.py:37] Route: /version, Methods: GET
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /pooling, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /classify, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /score, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /rerank, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /invocations, Methods: POST
INFO 06-22 22:53:12 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [3256227]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:53280 - "GET /v1/models HTTP/1.1" 200 OK
[INFO] ✅ minicpm API is ready!
[INFO] Starting AEQA evaluation on GPU 1 (3dmem env)...
00:00:00 - ***** Running exp_eval_aeqa *****
00:00:00 - Total number of questions: 41
00:00:00 - number of questions after splitting: 41
00:00:00 - question path: data/aeqa_questions-41.json
00:00:00 - Load YOLO model yolov8x-world.pt successful!
00:00:03 - Load SAM model sam_l.pt successful!
00:00:03 - Loaded ViT-B-32 model config.
00:00:04 - Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
00:00:05 - Load CLIP model successful!
00:00:05 - Question 00c2be2a-1377-4fae-a889-30936b7890c3 already processed
00:00:05 - Question 013bb857-f47d-4b50-add4-023cc4ff414c already processed
00:00:05 - Question 01fcc568-f51e-4e12-b976-5dc8d554135a already processed
00:00:05 - Question 0df60236-15ad-4166-a31a-a98d14214fdb already processed
00:00:05 - Question 109eaad4-6e68-4da1-8f98-a0d8589ec26d already processed
00:00:05 - Question 1b36e675-74ff-46ad-8caa-c33da46a5a67 already processed
00:00:05 - Question 1dcdd225-eba2-4ba1-97b6-c4cdc7ca4e9b already processed
00:00:05 - Question 30dc765d-80c3-4901-9c69-65e6b48e254a already processed
00:00:05 - Question 3a5be057-47d2-4f78-98a9-729ef19b3d8b already processed
00:00:05 - Question 45a5e082-a9e9-47ca-a036-dfafba92b16c already processed
00:00:05 - Question 48d8aa7f-61cb-469b-9b6d-2549d1210281 already processed
00:00:05 - Question 4cc4212e-0db2-421f-8bb5-93817e58f9b4 already processed
00:00:05 - Question 4dbd213e-56cd-481a-8ff5-ed9a8d636dbc already processed
00:00:05 - Question 67cd7145-4b1f-4b2a-a698-8e4738cb7c41 already processed
00:00:05 - Question 6852b358-4820-471d-9263-d32ef0cecd0b already processed
00:00:05 - Question 6d132959-fd48-4fef-a736-4e5853849547 already processed
00:00:05 - Question 7ebac357-a338-4ce0-975a-62141e90a3c3 already processed
00:00:05 - Question 90ab6389-d85e-42ad-b44a-af4849da2631 already processed
00:00:05 - Question 911693d9-2d28-4ff2-83a9-c67b83753831 already processed
00:00:05 - Question 9b2d06e5-ca78-4519-a9ca-75c06209b770 already processed
00:00:05 - Question a36ab369-6f78-4311-a943-b6862cd28b55 already processed
00:00:05 - Question a5c5bb29-700a-4ef5-b17d-aaa47bb0ef3f already processed
00:00:05 - Question a605c40f-96e7-4bec-a1cb-6d48e88e39cd already processed
00:00:05 - Question ae19adeb-498a-4814-b955-e0af05623f9b already processed
00:00:05 - Question b05e7b30-6a4d-4381-9d05-a42ed0c90e30 already processed
00:00:05 - Question b93ea8d4-4b9a-46a3-b9b4-3d79c5ce074e already processed
00:00:05 - Question ba5f1c9b-9a41-4a84-829b-f9b8ccd19b69 already processed
00:00:05 - Question bd5e9e4e-c6be-40e9-a923-fcc6aa321947 already processed
00:00:05 - Question c1b2ccf5-b56d-4ced-9cec-eaf62fedc675 already processed
00:00:05 - Question cbffc0cd-04aa-4686-97bf-887c0dc840bd already processed
00:00:05 - Question d3742804-8363-4346-a622-5bcaeffb25e9 already processed
00:00:05 - Question d4c10718-fd57-4db0-93c1-b54deb4b1b25 already processed
00:00:05 - Question d8183087-f3dd-47c1-b985-733923edc4a0 already processed
00:00:05 - Question de038605-c441-4a30-968b-7815bad3a3c9 already processed
00:00:05 - Question dfdc3b36-d98f-42a7-b2ea-dceb4af1794a already processed
00:00:05 - Question e0d20472-8fa6-4e8d-880d-22d4eed3fbb8 already processed
00:00:05 - Question e6fb0c2e-5f92-4835-ba38-6af958b7a1d3 already processed
00:00:05 - Question f17869a2-2a4d-4ce4-b262-cb69618e3394 already processed
00:00:05 - Question f2063c53-72d8-4cd8-b2cb-78ceee86449d already processed
00:00:05 - Question f5a17a09-ce4b-4123-bf40-d2239cf38cb8 already processed
00:00:05 - Question fc9d2a18-6197-4c8b-abd8-be0c493e5450 already processed
00:00:05 - Average number of filtered snapshots: 1.951219512195122
00:00:05 - Average number of total snapshots: 5.487804878048781
00:00:05 - Average number of total frames: 7.487804878048781
00:00:05 - All scenes finish
len(success_list) 31
len(fail_list) 10
len(gpt_answer_list) 41
len(n_filtered_snapshots_list) 41
len(n_total_snapshots_list) 41
len(n_total_frames_list) 41
[INFO] AEQA finished. Killing vLLM server (PID=3256227)...
=== JOB END ===
